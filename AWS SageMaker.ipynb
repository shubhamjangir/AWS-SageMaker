{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f984270",
   "metadata": {},
   "source": [
    "# AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6843a",
   "metadata": {},
   "source": [
    "## 1-Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20703ec",
   "metadata": {},
   "source": [
    "Amazon SageMaker is a cloud machine-learning platform that was launched in November 2017. SageMaker enables developers to create, train, and deploy machine-learning models in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc934cbf",
   "metadata": {},
   "source": [
    "#### How it Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9f4da",
   "metadata": {},
   "source": [
    "Label-Labeling jobs for highly accurate training datasets within Amazon SageMaker, using active learning and human labeling.     \n",
    "Process of tagging and detecting data samples. Process of attaching meaning to the data.     \n",
    "Raw data --> Labelling -->Lablers(choose from different workforce)-->Assistive labelling-->Accurate Training Model     \n",
    "Improves data label accuracy, Easy to use, Reduces cost by 70% and Choose your Workforce.   \n",
    "\n",
    "Build-Connect to other AWS services and transform data in Amazon SageMaker notebooks.    \n",
    "Train-Use Amazon SageMaker's algorithms and frameworks, or bring your own, for distributed training.    \n",
    "Tune-Amazon SageMaker automatically tunes your model by adjusting multiple combinations of algorithm parameters.   \n",
    "Deploy-Once training is completed, models can be deployed to Amazon SageMaker endpoints, for real-time predictions.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17611907",
   "metadata": {},
   "source": [
    "#### Benefits and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07feab1b",
   "metadata": {},
   "source": [
    "Labeling raw data with active learning- Continuosly learn and improve between machine and human.     \n",
    "Highly accurate training datasets-Active learning models from Amazon SageMaker Ground Truth provide a very high level of consistency and accuracy for training datasets.     \n",
    "Fully managed notebook instances-For training data exploration and preprocessing, Amazon SageMaker provides fully managed instances running Jupyter notebooks that include example code for common model training and hosting exercises.  \n",
    "Highly optimized machine learning algorithms-Amazon SageMaker installs high-performance, scalable machine learning algorithms optimized for speed, scale, and accuracy, to run on extremely large training datasets.  \n",
    "One-click training-When you're ready to train in Amazon SageMaker, simply indicate the type and quantity of instances you need and initiate training with a single click.      \n",
    "Deployment without engineering effort-After training, SageMaker provides the model artifacts and scoring images to you for deployment to Amazon EC2 or anywhere else.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982eb06",
   "metadata": {},
   "source": [
    "## 2-Creating free-tier AWS account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddae98",
   "metadata": {},
   "source": [
    "## 3-Notebook Instance/SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1789eb8",
   "metadata": {},
   "source": [
    "It is same as Jupyter Notebook/Jupyter lab hosted in AWS Cloud you can choose the environment also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d501b",
   "metadata": {},
   "source": [
    "#### AWS Elastic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a61bf",
   "metadata": {},
   "source": [
    "Amazon Elastic Inference allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Sagemaker instances or Amazon ECS tasks, to reduce the cost of running deep learning inference by up to 75%. Amazon Elastic Inference supports TensorFlow, Apache MXNet, PyTorch and ONNX models.\n",
    "\n",
    "Inference is the process of making predictions using a trained model. In deep learning applications, inference accounts for up to 90% of total operational costs for two reasons. Firstly, standalone GPU instances are typically designed for model training - not for inference. While training jobs batch process hundreds of data samples in parallel, inference jobs usually process a single input in real time, and thus consume a small amount of GPU compute. This makes standalone GPU inference cost-inefficient. On the other hand, standalone CPU instances are not specialized for matrix operations, and thus are often too slow for deep learning inference. Secondly, different models have different CPU, GPU, and memory requirements. Optimizing for one resource can lead to underutilization of other resources and higher costs.\n",
    "\n",
    "Amazon Elastic Inference solves these problems by allowing you to attach just the right amount of GPU-powered inference acceleration to any EC2 or SageMaker instance type or ECS task, with no code changes. With Amazon Elastic Inference, you can choose any CPU instance in AWS that is best suited to the overall compute and memory needs of your application, and then separately configure the right amount of GPU-powered inference acceleration, allowing you to efficiently utilize resources and reduce costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73210d3d",
   "metadata": {},
   "source": [
    "#### IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f641d",
   "metadata": {},
   "source": [
    "You can use IAM to securely control individual and group access to your AWS resources. You can create and manage user identities (\"IAM users\") and grant permissions for those IAM users to access your resources. You can also grant permissions for users outside of AWS ( federated users).\n",
    "Suppose if we are working on a bank appliation and we want to access a specific S3 bucket or all the buckets we can crreate and give permission to an individual or group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe74abe",
   "metadata": {},
   "source": [
    "## 4-Importing Libraries and Creating S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3 #AWS SDK for Python (Boto3) to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3).\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri #estimators to train your model.\n",
    "from sagemaker.session import s3_input, Session #Manage interactions with the Amazon SageMaker APIs and any other AWS services needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the S3 bucket\n",
    "bucket_name = 'bankapplication' \n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating S3 bucket using Notebook instance in the region eu-west-1\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'eu-west-1':\n",
    "        s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration={'LocationConstraint': 'eu-west-1'})\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set an output path where the trained model will be saved\n",
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69b45b",
   "metadata": {},
   "source": [
    "## 5-Downloading The Dataset And Storing in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the dataset\n",
    "import pandas as pd\n",
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33460a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Train And Test Into Buckets\n",
    "import os\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)],axis=1).to_csv('train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Into Buckets\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2f90f",
   "metadata": {},
   "source": [
    "## 6-Building the Model(Xgboost-Inbuild Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "#Specify the repo_version depending on your preference.\n",
    "container = get_image_uri(boto3.Session().region_name,\n",
    "                          'xgboost', \n",
    "                          repo_version='1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6039aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f52dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_name=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.2xlarge', \n",
    "                                          train_volume_size=5, # 5 GB \n",
    "                                          output_path=output_path,\n",
    "                                          train_use_spot_instances=True,\n",
    "                                          train_max_run=300,\n",
    "                                          train_max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc963b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3107a",
   "metadata": {},
   "source": [
    "## 7-Prediction of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94838623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "xgb_predictor.serializer = csv_serializer # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd456731",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45966d9a",
   "metadata": {},
   "source": [
    "## 8-Deleting the endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07614c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
